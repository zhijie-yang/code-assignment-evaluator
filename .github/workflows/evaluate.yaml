name: Evaluate
on:
  workflow_call:
    inputs:
      format:
        description: 'Run format checks'
        type: boolean
        default: false
      test:
        description: 'Run unit and integration tests'
        type: boolean
        default: true
      test-root:
        description: 'Root directory for tests'
        type: string
        default: './tests'
  workflow_dispatch:
    inputs:
      format:
        description: 'Run format checks'
        type: boolean
        required: true
        default: false
      test:
        description: 'Run unit and integration tests'
        type: boolean
        required: true
        default: true
      test-root:
        description: 'Root directory for tests'
        type: string
        default: './tests'

jobs:
  format:
    runs-on: ubuntu-latest
    if: ${{ inputs.format }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v5

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install black

      - name: Run black
        run: |
          black --check .

  test:
    runs-on: ubuntu-latest
    if: ${{ inputs.test }}
    env:
      PYTEST_RESULT_PATH: pytest_results.xml
    steps:
      - name: Checkout evaluator repository
        uses: actions/checkout@v5
        with:
          repository: zhijie-yang/code-assignment-evaluator
          path: evaluator

      - name: Checkout repository
        uses: actions/checkout@v5

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest

      - name: Run pytest
        continue-on-error: true
        run: |
          python3 -m pytest --junit-xml "${{ env.PYTEST_RESULT_PATH }}" "${{ inputs.test-root }}"

      - name: Generate Summary
        if: ${{ !cancelled() }}
        run: |
          python3 -m evaluator.tools.junit_to_markdown --input-junit "${{ env.PYTEST_RESULT_PATH }}" >> $GITHUB_STEP_SUMMARY
